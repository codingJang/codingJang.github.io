# Day 11: 다변수 경사하강법

### CH2 | Gradient descent, how neural networks learn

[https://www.youtube.com/watch?v=IHZwWFHWa-w](https://www.youtube.com/watch?v=IHZwWFHWa-w)

### 문제 2.1

(a) 비디오에서 예시로 사용한 학습 데이터는 어떻게 구성되어 있는가?

*Note. 숫자 분류기 예제에서 사용된 설정은 신경망이 (사람이 만든) **데이터-레이블 쌍**이 학습의 방향성을 지도한다는 의미에서 “지도 학습” 설정이라고 한다. 또한, 신경망이 **레이블이 없는** 데이터에서 패턴을 추출하는 “비지도 학습” 설정도 있다.*

(b) 비용 함수의 역할은 무엇인가?

*Note. 비용 함수는 “손실 함수” 또는 “목적 함수”라고 부르기도 한다.*

### 문제 2.2

(a) 1차원에서 경사 하강법이 어떻게 작동하는지 설명하시오 (매개변수가 $1$개인 경우). 비용 함수가 $C(x)$로 표시되고 $\frac{dC}{dx}(x)>0$인 경우, $C(x)$를 감소시키기 위해 매개변수 $x$를 어떻게 업데이트해야 하는가? 단, 스텝 크기가 충분히 작다고 가정한다.

(b) 2차원에서 경사 하강법이 어떻게 작동하는지 설명하시오 (매개변수가 $2$개인 경우). 비용 함수가 $C(x, y)$로 표시되고, $\nabla C(x,y)=(1,2)$인 경우, $C(x, y)$를 감소시키기 위해 매개변수 $x$와 $y$를 어떻게 업데이트해야 하는가? 단, 스텝 크기가 충분히 작다고 가정한다.

(c) $n$차원에서 경사 하강법이 어떻게 작동하는가? 즉 매개변수가 $n$개인 경우로의 일반화는 어떻게 설명 가능하며 (a), (b)와 작동 원리는 동일한가?

### 문제 2.3

(a) ① 지역 최소값과 ② 전역 최소값 두 개념을 각각 설명하라.

(b) 비용 함수 $C$에 대한 경사 하강법이 여러 스텝 진행된 이후 매개변수 $\mathbf{p}=(p_1, p_2,  \cdots, p_n)$가 특정 지점 $\mathbf{p}^{(\infty)}$에 수렴했다고 가정하자. $\mathbf{p}^{(\infty)}$가 비용 함수 $C$의 전역 최소값이라고 확신할 수 있는가?