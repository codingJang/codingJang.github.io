<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://codingjang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://codingjang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-01T15:31:54+00:00</updated><id>https://codingjang.github.io/feed.xml</id><title type="html">blank</title><subtitle>Yejun Jang is an AI researcher specializing in reinforcement learning and deep learning at Seoul National University. </subtitle><entry><title type="html">과외 자료 모음</title><link href="https://codingjang.github.io/blog/2024/tutoring-materials-index/" rel="alternate" type="text/html" title="과외 자료 모음"/><published>2024-07-01T10:00:00+00:00</published><updated>2024-07-01T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/tutoring-materials-index</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/tutoring-materials-index/"><![CDATA[<h2 id="전략">전략</h2> <p><a href="/assets/blog/tutoring-materials/%EC%A0%84%EB%9E%B5%20%EC%96%B4%EB%A0%A4%EC%9A%B4%20%EB%AC%B8%EC%A0%9C%EA%B0%80%20%ED%92%80%EB%A6%AC%EB%8A%94%20%EC%9D%B4%EC%9C%A0%20161f0f24f931809c9bf9e8ee042d6da7.md">전략 : 어려운 문제가 풀리는 이유</a></p> <p><a href="/assets/blog/tutoring-materials/%EC%A0%84%EB%9E%B5%20%ED%8F%89%EB%A9%B4%20%EA%B8%B0%ED%95%98%ED%95%99%20161f0f24f93180428dcbf05b9af4a99d.md">전략 : 평면 기하학</a></p> <p><a href="/assets/blog/tutoring-materials/Tips%20and%20Tricks%20161f0f24f931804f9613f61ca4debc9f.md">Tips and Tricks</a></p> <p><a href="/assets/blog/tutoring-materials/Tutoring%20161f0f24f93180678c82cdc31466efbc.md">Tutoring</a></p> <h2 id="수학적-모델링">수학적 모델링</h2> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20COVID-19%EA%B3%BC%20%EB%93%B1%EB%B9%84%EC%88%98%EC%97%B4%20161f0f24f931806582a9e4e02b00dfc4.md">도전 과제 : COVID-19과 등비수열</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%EC%88%98%EC%97%B4%EA%B3%BC%20%EC%9E%85%EC%B2%B4%EB%8F%84%ED%98%95%EC%9D%98%20%EB%B6%80%ED%94%BC%20161f0f24f931809e80d0e567096c6ae1.md">도전 과제 : 수열과 입체도형의 부피</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%EC%9A%94%EC%84%B8%ED%91%B8%EC%8A%A4%20%EB%AC%B8%EC%A0%9C%202636215cfa7f4b16a3a2eb9310b4b56a.md">도전 과제 : 요세푸스 문제</a></p> <h2 id="기하학">기하학</h2> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%201%EC%B4%88%EC%9D%98%20%EA%B8%B8%EC%9D%B4,%201%EB%AF%B8%ED%84%B0%EC%9D%98%20%EC%8B%9C%EA%B0%84%20161f0f24f93180a9a487cc6e710f1c8e.md">도전 과제 : 1초의 길이, 1미터의 시간</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%EC%9E%90%EC%99%80%20%EC%BB%B4%ED%8D%BC%EC%8A%A4%20161f0f24f93180c5b98ef3a3182bdc48.md">도전 과제 : 자와 컴퍼스</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%EC%82%BC%EA%B0%81%ED%95%A8%EC%88%98%20161f0f24f931803bb39df191a19a659a.md">도전 과제 : 삼각함수</a></p> <p><a href="/assets/blog/tutoring-materials/TEST%2001%20%EB%8F%84%ED%98%95%EC%9D%98%20%EB%8B%AE%EC%9D%8C%20161f0f24f93180d29f25c8f66c09ee22.md">TEST 01 : 도형의 닮음</a></p> <h2 id="조합론">조합론</h2> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%EA%B8%B8%20%EC%B0%BE%EA%B8%B0%20161f0f24f93180d49552e6002b713243.md">도전 과제 : 길 찾기</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%ED%95%98%EB%85%B8%EC%9D%B4%EC%9D%98%20%ED%83%91%20161f0f24f9318047ba72e83be080bb65.md">도전 과제 : 하노이의 탑</a></p> <h2 id="정수론">정수론</h2> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%93%9C%20%ED%98%B8%EC%A0%9C%EB%B2%95%20161f0f24f93180a49e4ad85867e9ab86.md">도전 과제 : 유클리드 호제법</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%8F%84%EC%A0%84%20%EA%B3%BC%EC%A0%9C%20%ED%8C%B0%EB%A6%B0%EB%93%9C%EB%A1%AC%20161f0f24f93180f1981ff081072da407.md">도전 과제 : 팰린드롬</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%B3%B4%EC%B6%A9%20%EC%9E%90%EB%A3%8C%20%EC%B5%9C%EB%8C%80%EA%B3%B5%EC%95%BD%EC%88%98%EC%99%80%20%EC%B5%9C%EC%86%8C%EA%B3%B5%EB%B0%B0%EC%88%98%20161f0f24f931802cbe25e788d3d23642.md">보충 자료 : 최대공약수와 최소공배수</a></p> <h2 id="대수학">대수학</h2> <p><a href="/assets/blog/tutoring-materials/%EA%B0%9C%EB%85%90%20%EC%9D%B5%ED%9E%88%EA%B8%B0%20%EC%9D%B8%EC%88%98%20%EC%A0%95%EB%A6%AC%EB%A5%BC%20%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EC%9D%B8%EC%88%98%EB%B6%84%ED%95%B4%20161f0f24f931800ebd40d3749a386801.md">개념 익히기 : 인수 정리를 이용한 인수분해</a></p> <p><a href="/assets/blog/tutoring-materials/%EA%B0%9C%EB%85%90%20%EC%9D%B5%ED%9E%88%EA%B8%B0%20%EC%A1%B0%EB%A6%BD%EC%A0%9C%EB%B2%95%EC%9D%98%20%EC%9B%90%EB%A6%AC%20161f0f24f93180848b19e20bd320fc93.md">개념 익히기 : 조립제법의 원리</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%B3%B4%EC%B6%A9%20%EC%9E%90%EB%A3%8C%20%EC%9C%A0%EB%A6%AC%EC%88%98%EC%9D%98%20%EC%82%AC%EC%B9%99%20%EC%97%B0%EC%82%B0%20161f0f24f9318037bf63fc6fbb5b4222.md">보충 자료 : 유리수의 사칙 연산</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%B3%B4%EC%B6%A9%20%EC%9E%90%EB%A3%8C%20%EC%86%8C%EA%B8%88%EB%AC%BC%EC%9D%98%20%EB%86%8D%EB%8F%84%208afc38377483482faa71a9b1f953c3fb.md">보충 자료 : 소금물의 농도</a></p> <p><a href="/assets/blog/tutoring-materials/%EB%B3%B4%EC%B6%A9%20%EC%A7%88%EB%AC%B8%20%EC%BC%A4%EB%A0%88%20%EB%B3%B5%EC%86%8C%EC%88%98%20161f0f24f93180cc954ef0da7965b6ae.md">보충 질문 : 켤레 복소수</a></p> <p><a href="/assets/blog/tutoring-materials/TEST%2002%20%EB%8B%A4%ED%95%AD%EC%8B%9D%20161f0f24f931803794d1dba6318480b2.md">TEST 02 : 다항식</a></p>]]></content><author><name></name></author><category term="education"/><category term="education"/><category term="tutoring"/><category term="mathematics"/><category term="korean"/><summary type="html"><![CDATA[수학 과외 및 교육 자료 모음 - 전략, 수학적 모델링, 문제 풀이]]></summary></entry><entry><title type="html">AI를 적용하기 좋은 주제에 관하여</title><link href="https://codingjang.github.io/blog/2024/ai-application-topics/" rel="alternate" type="text/html" title="AI를 적용하기 좋은 주제에 관하여"/><published>2024-06-15T10:00:00+00:00</published><updated>2024-06-15T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/ai-application-topics</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/ai-application-topics/"><![CDATA[<p>AI를 적용하기 좋은 주제는 주로 양질의 학습 데이터가 제공되는 경우이다.</p> <h3 id="1000개-이상의-데이터포인트를-확보하자">1000개 이상의 데이터포인트를 확보하자</h3> <p>(보통 인공지능 모델을 학습하는데 있어서) 데이터포인트가 최소 1000개 이상이 필요하다. 생각보다 적은 인원으로 데이터를 수작업으로 만들 수 있는 경우도 존재한다.</p> <p>예를 들어, (물론 이미 인터넷 상에 공개되어 있는 데이터셋이지만) 얼굴 인식 데이터셋을 직접 만들고 싶다면 사람의 얼굴이 있는 부분을 색칠하면 된다. 인원이 3명이면, 한 사람당 이미지 333개에서 얼굴을 색칠하는 노가다 작업을 하면 학습을 시작할 수 있다.</p> <h3 id="데이터-얻기-1--공개된-데이터셋의-활용">데이터 얻기 1 | 공개된 데이터셋의 활용</h3> <p>물론 시간을 절약하는 가장 좋은 방법은 이미 공개되어 있는 데이터셋을 활용하는 것이다. 공개된 데이터셋은 (분야 이름) + dataset으로 검색하면 잘 나오고, 이미 선행 연구에서 사용 중인 표준 데이터셋이 존재한다면 이를 활용하는 것이 일반적이다. 표준 데이터셋을 활용할 경우 객관적인 성능 비교가 가능하다는 측면에서 장점이 있다.</p> <h3 id="데이터-얻기-2--직접-데이터셋을-구축하는-방법">데이터 얻기 2 | 직접 데이터셋을 구축하는 방법</h3> <p>하지만, 우리가 해결하려는 문제를 대변하는 좋은 데이터셋이 없다면 직접 구축하는 수밖에 없다. 귀찮은 작업이더라도, 새로운 분야에 인공지능을 접목하는 연구에서는 필수적인 작업이다. 오히려, 데이터셋을 구축하여 오픈소스로 공개한 뒤 해당 데이터셋이 분야 표준으로 자리잡는다면, 연구의 기여를 인정받을 수 있다.</p> <h3 id="데이터-얻기-3--기업체-또는-아르바이트를-활용하여-아웃소싱">데이터 얻기 3 | 기업체 또는 아르바이트를 활용하여 아웃소싱</h3> <p>돈이 충분하다면, 학습 데이터셋을 구축하는 작업을 외주로 맡길 수 있다. 데이터 레이블링(지도학습 데이터셋 구축)을 전문으로 하는 회사들이 있다. 또는, 데이터레이블링을 단기 알바 형태로 만들어 크라우드소싱(crowdsourcing)을 통해 해결하는 경우도 있다.</p> <h3 id="학습할-수-있는-데이터에는-어떤-것이-있는가">학습할 수 있는 데이터에는 어떤 것이 있는가?</h3> <p>반드시 숫자로 표현할 수 있는 데이터가 아니어도 괜찮다. 영어, 한국어 등 사람이 평소에 사용하는 언어인 자연어(natural language)로 표현된 데이터, 이미지/동영상 데이터 등 디지털화할 수 있는 정보는 대부분 학습 가능하다.</p> <p>그러나, 디지털화가 제대로 진행되지 않은 정보의 처리를 요구하는 경우 학습 데이터 구축에 난항을 겪을 수 있다. 예를 들어, 아직 그 어느 기관에서도 설문조사를 통해 질의한 적이 없는 질문에 대한 답변을 학습시키고 싶다면, 데이터셋을 구축하는 것만으로도 수개월이 걸릴 수 있다.</p> <p>또는, 데이터 생성에 필요한 센서가 대중화되어 있지 않은 경우 제공 받을 수 있는 학습 데이터의 양에 근본적인 제약이 걸릴 수 있다. 예를 들어, 음식을 맛보는 센서는 지금까지 스마트폰에 탑재되었던 적이 없었기 때문에, 학습 데이터를 얻으려면 음식의 구성 성분을 분석할 수 있는 전문 실험 장비를 보유한 회사/연구소에 의뢰하여야 한다.</p> <h3 id="학습-데이터의-대표성">학습 데이터의 대표성</h3> <p>데이터 수집 과정에서 편향이 발생하지는 않았는지 충분히 살펴보아야 한다. 당연한 이야기이지만, 서울 시민을 표본으로 한 여론 조사 데이터셋을 학습한 인공지능이 대한민국에서 선출될 대통령을 예상할 수는 없다.</p> <p>데이터셋을 만드는 사람은 일관되고 정확한 방식으로 데이터셋을 생성해야 한다. 기계학습에서의 목적은 데이터의 종합적인 경향성과 패턴을 파악하는 것이기에 어느 정도의 실수는 용인되지만, 실수가 너무 잦거나 아웃라이어(outlier)에 해당하는 극단적인 샘플이 섞여들어갈 경우 학습이 제대로 진행되지 않을 수 있다.</p> <p>또한 너무 오래된 데이터셋을 활용하면 예측 정확도가 떨어질 수 있다. 예를 들어, 1900년대 주식 데이터셋만 가지고 있을 경우, 2000년대가 되어서야 대중화된 디지털 기기의 존재를 알 수 없기 때문에 이것으로 트레이딩 자동화를 시도하겠다는 것은 터무니없는 이야기라고 할 수 있겠다.</p> <p>반대로, 전쟁과 같이 비교적 큰 타임스케일(time-scale)로 발생하는 사건을 예측하는 인공지능을 만들고자 할 때, 2000년대 이후의 최신 데이터만을 이용해서 학습하는 것은 적절하지 못할 것이다. 충분히 많은 전쟁에 대한 정보가 누적되어야 하는데 2000년부터 지금까지 대규모 전쟁이 발발한 적은 없기 때문이다.</p> <p>학습 데이터가 시뮬레이션을 통해 얻어질 경우, 해당 시뮬레이션이 충분히 믿을만한 결과를 제공하는지에 대해 엄밀하게 분석해보아야 한다. 예를 들어, 학습 데이터를 경제 시뮬레이션을 통해 생성했는데, 해당 시뮬레이션이 현실을 제대로 대변하지 못한다면, 학습이 되더라도 학습의 결과가 시뮬레이션 상에서만 만족스럽고 정작 현실 상황에 적용 불가능할 수 있다. 이를 심-투-리얼 갭(sim-to-real gap)이라고 한다.</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="machine-learning"/><category term="data-science"/><category term="korean"/><summary type="html"><![CDATA[인공지능을 효과적으로 적용하기 위한 데이터셋 구축 및 선정 가이드]]></summary></entry><entry><title type="html">벡터와 벡터 공간</title><link href="https://codingjang.github.io/blog/2024/vectors-and-vector-spaces/" rel="alternate" type="text/html" title="벡터와 벡터 공간"/><published>2024-05-15T10:00:00+00:00</published><updated>2024-05-15T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/vectors-and-vector-spaces</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/vectors-and-vector-spaces/"><![CDATA[<h2 id="벡터란-무엇인가">벡터란 무엇인가?</h2> <p>많은 사람들은 벡터를 크기와 방향을 가지는 화살표의 개념으로 알고 있다. 이러한 관점이 틀린 것은 아니며, 오히려 화살표로 벡터를 표기한 덕분에 많은 사람들이 더 쉽게 벡터를 이해하고 있는 것이라고 할 수 있다.</p> <p>그러나, 수학자들은 “크기와 방향을 가지는 화살표”라는 벡터의 정의가 다소 <strong>엄밀하지 않다</strong>고 본다. 애초에 “화살표”라는 것이 수학적으로 엄밀한 표현이 아니니 말이다. “방향”이나 “크기”도 현실에서 그것이 무엇을 의미하는지는 어느 정도 명확하지만, 좌표계를 정의하기 전부터 “방향”과 “크기”를 논하는 것은 논리적으로 붕 떠있는 느낌이 든다.</p> <p>그래서 수학자들은 단순한 화살표에 머물던 벡터의 개념을 더 넓은 범위에서 확장하여 화살표 말고도 다양한 대상을 표현할 수 있기를 바랬다. 그들의 목표는 우리가 알고 있던 수학적 개념들 중 <strong>선형 결합에 대해 닫혀 있는</strong> 모든 대상들을 표현하는 것이었다.</p> <p>예를 들어, $\mathbb{R} \rightarrow \mathbb{R}$인 미분 가능한 함수들의 집합 $\mathcal{F}$의 원소 $f$와 $g$가 있다고 하자. 수식으로 표현하면 아래와 같다:</p> \[\mathcal{F} =\{ \mathbb{R} \rightarrow \mathbb{R}인\; 미분\; 가능한\; 함수\} \newline f,\;g \in \mathcal{F}\] <p>$f$와 $g$를 선형 결합한 함수 $af + bg$를 아래와 같이 정의하자:</p> \[임의의\; a, b \in \mathbb{R}와 \;임의의\; f,\;g \in \mathcal{F}에\; 대해,\newline \; af + bg는\; \mathbb{R} \rightarrow \mathbb{R}이고 \;x \mapsto af(x)+bg(x)인\; 함수로 \; 정의한다\newline\] <p>그러면 당연히 선형 결합한 함수도 미분 가능하므로 $af + bg \in \mathcal{F}$가 되고, 이를 집합 $\mathcal{F}$가 선형 결합 연산에 대해 닫혀 있다고 한다. 따라서 집합 $\mathcal{F}$는 실수체 $\mathbb{R}$ 위에서 벡터 공간을 이룬다. 사실 엄밀히는 조건 몇 가지가 더 필요하므로, 아래에서 한 번 살펴보자.</p> <h2 id="벡터-공간의-정의">벡터 공간의 정의</h2> <h3 id="실벡터-공간의-정의">실벡터 공간의 정의</h3> <p>실수 집합 위에서 정의되는 벡터 공간은 아래를 만족하는 집합 $V$, 덧셈 연산 $+$, 스칼라배 연산 $:\cdot:$의 모임 $\left&lt;V,:+,:\cdot \right&gt;$로 정의되고, 줄여서 $V$로 표기한다. ($+$는 벡터 공간에 대해 닫혀 있는 이항 연산자이고, $:\cdot:$은 집합 $\mathbb{R}$의 원소와 벡터 공간 $V$의 원소를 연산하여 다시 $V$의 원소를 돌려주는 연산자이다.)</p> <ol> <li>$V$는 덧셈 연산 $+$에 대해 가환군을 이룬다.</li> <li>$V$에 속한 모든 $v$와 실수 집합 $\mathbb{R}$에 속한 모든 $\lambda,\mu$에 대해 $\lambda(\mu v)=(\lambda\mu)v$</li> <li>$V$에 속한 모든 $v$와 실수 집합 $\mathbb{R}$에 속한 모든 $\lambda,\mu$에 대해 $(\lambda+\mu) v=\lambda v + \mu v$</li> <li>$V$에 속한 모든 $v,w$와 실수 집합 $\mathbb{R}$에 속한 모든 $\lambda$에 대해 $\lambda(v+w)=\lambda v + \lambda w$</li> <li>$V$에 속한 모든 $v$에 대해 $1v=v$ (여기서 $1$은 실수 집합 $\mathbb{R}$의 곱셈에 대한 항등원이다)</li> </ol> <h3 id="벡터-공간의-정의-1">벡터 공간의 정의</h3> <p>위의 정의에서 실수 집합 $\mathbb{R}$ 대신 임의의 체 $F$에 대해 정의하면 일반적인 벡터 공간의 정의를 얻는다.</p> <h3 id="벡터의-정의">벡터의 정의</h3> <p>벡터는 벡터 공간의 원소이다.</p> <p>수학자들은 벡터 공간의 원소를 벡터로 부른다. 이러면 벡터가 굳이 화살표여야 할 필요가 없다 - 즉, $\mathbb{R} \rightarrow \mathbb{R}$인 미분가능한 함수는 모두 벡터인 것이다. 다항식에 대해서도 같은 논리가 성립한다는 것을 확인할 수 있다. 즉, 미분가능한 함수, 다항식 모두 넓은 범위에서는 벡터이다. 다항식과 함수를 벡터로 표현한다니, 처음 공부하는 사람이라면 이게 말이 되는지 의문을 품을 수도 있겠다. 하지만, 우리는 이미 화살표로 나타내어진 벡터에 대한 많은 연산들을 정의하고 사용해왔다. 벡터의 덧셈, 덧셈의 교환 법칙, 적당한 실수를 곱하는 연산, 두 벡터를 내적하는 연산, 크기를 구하는 연산 … 이런 것들은 고등학교 과정에서도 전부 등장하는 내용들이고, 물리학을 배우면 자연스럽게 사용하게 되는 개념들이다. 우리가 지금껏 사용해왔던 벡터의 특징을 살펴보고, 이를 바탕으로 발견한 특징들을 일반화하는 과정을 거치면 함수도 벡터로 충분히 생각할 수 있다.</p> <p>벡터 공간은 공간을 정의하는데 사용된 집합과 같은 의미로 사용되기도 하는데, 굳이 집합이 아닌 <strong>공간(space)</strong>이라는 용어를 구분하는 이유는 집합 내의 원소들에 대해 일정한 규칙, 즉 <strong>연산(operation)</strong>이 정의되어 있기 때문이다. 한 집합 내의 원소들에 대해 연산이 잘 정의되어 있으면 집합이 <strong>구조(structure)</strong>를 가진다고 하고 구조를 가지는 집합을 <strong>공간(space)</strong>이라고 지칭한다.</p> <p>이제는 벡터를 ‘순서쌍’이나 ‘화살표’와 같이 구체적인 형태를 나타내어주지 않아도, <strong>어떤 집합에 대해 위에서 언급한 연산들이 정의가 되어 있고 이 연산들이 위와 같은 다섯 개의 규칙을 따른다면 이 집합과 연산의 모임은 ‘자동적으로’ 벡터 공간이 된다.</strong> 구체적인 예시로 정의되지 않고 가장 일반적인 형태로 정의되기 때문에, 이를 <strong>벡터 공간의 추상적 정의(abstract definition of a vector space)</strong>라고도 부른다.</p>]]></content><author><name></name></author><category term="mathematics"/><category term="mathematics"/><category term="linear-algebra"/><category term="vectors"/><category term="korean"/><summary type="html"><![CDATA[선형대수학의 기초 - 벡터와 벡터 공간의 정의]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://codingjang.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">군의 정의와 예시</title><link href="https://codingjang.github.io/blog/2024/group-theory-basics/" rel="alternate" type="text/html" title="군의 정의와 예시"/><published>2024-05-10T10:00:00+00:00</published><updated>2024-05-10T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/group-theory-basics</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/group-theory-basics/"><![CDATA[<h2 id="군의-정의">군의 정의</h2> <p>집합 $G$와 $G$의 원소들에 대해 정의된 닫혀 있는 이항연산자 $<em>: G\times G \rightarrow G$에 대해, 다음 조건을 만족하면 $\left&lt;G,</em> \right&gt;$는 군을 이룬다:</p> <ol> <li>항등원이 존재한다. 즉, 모든 $a \in G$에 대해 $a * e=e * a=a$를 만족하는 $e \in G$가 존재한다.</li> <li>모든 원소에 대한 역원이 존재한다. 즉, 모든 $a \in G$에 대해 $a * a^{-1}=a^{-1} * a=e$를 만족하는 $a^{-1} \in G$가 존재한다.</li> <li>결합 법칙이 성립한다. 즉, 모든 $a,b,c \in G$에 대해 $(a<em>b)</em>c=a<em>(b</em>c)$이다.</li> </ol> <h3 id="군의-예시"><strong>군의 예시</strong></h3> <p>집합 $G={0, 1, 2, 3, 4, 5}$에 대해 이항연산자 $\oplus$를 아래와 같이 정의한다:</p> \[a\oplus b=(a+b를\:6으로\:나눈\:나머지)\] <p>그렇다면 $\left&lt;G,\oplus \right&gt;$는 군을 이룬다. 그 이유는:</p> <ol> <li>$a \oplus b$는 $0$ 이상 $6$ 미만의 정수이므로 집합 $G$가 연산 $\oplus$에 대해 닫혀 있음은 자명하다.</li> <li>모든 $a \in G$에 대해 $a\oplus 0=0\oplus a=a$이기 때문에 항등원 $0$이 존재한다.</li> <li>$0$의 역원은 $0$이고, $1$의 역원은 $5$, $2$의 역원은 $4$, $3$의 역원은 $3$, $4$의 역원은 $2$, $5$의 역원은 $1$이다. 따라서 모든 원소에 대해 역원이 존재한다.</li> <li>$(a \oplus b) \oplus c = a \oplus (b \oplus c) = (a + b + c\textrm{를 6으로 나눈 나머지})$이므로 결합법칙이 성립한다.</li> </ol> <p>추가적으로, 교환법칙이 성립하므로 $\left&lt;G,\oplus \right&gt;$는 가환군(commutative group)을 이룬다. 가환군은 아벨군(Abelian group)이라고도 부른다.</p>]]></content><author><name></name></author><category term="mathematics"/><category term="mathematics"/><category term="group-theory"/><category term="abstract-algebra"/><category term="korean"/><summary type="html"><![CDATA[추상대수학의 기초 - 군론 입문]]></summary></entry><entry><title type="html">머신러닝은 양자역학 시뮬레이션을 가속화할 수 있을까?</title><link href="https://codingjang.github.io/blog/2024/ml-quantum-simulation/" rel="alternate" type="text/html" title="머신러닝은 양자역학 시뮬레이션을 가속화할 수 있을까?"/><published>2024-04-20T10:00:00+00:00</published><updated>2024-04-20T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/ml-quantum-simulation</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/ml-quantum-simulation/"><![CDATA[<h3 id="machine-learning-accelerated-quantum-mechanics-based-atomistic-simulations-for-industrial-applications">Machine learning-accelerated quantum mechanics-based atomistic simulations for industrial applications</h3> <h2 id="abstract">Abstract</h2> <ul> <li>ML methods has dramatically extended the applicability range of conventional QM-based simulations</li> <li>This paper will illustrate the benefits of ML methods in drug discovery and energy materials</li> </ul> <h1 id="introduction">Introduction</h1> <ul> <li>Computer-aided drug design <ul> <li>lower cost, decrease failure rates, speed up</li> </ul> </li> <li>Focuses on ML-accelerated QM methods and compares them with two main conventional approaches (MM and QM)</li> </ul> <h1 id="atomistic-simulation-methods">Atomistic Simulation Methods</h1> <ul> <li>PES (Potential Energy Surface)</li> <li>MD (Molecular Dynamics) or MC (Monte Carlo)</li> <li>The degree of physical approximation gives how more or less efficient a simulation is</li> <li>Non-emperical QM methods are the most accurate yet computationally most expensive</li> <li><strong>The choice of simulation depends on the four key aspects:</strong> <ul> <li>types of physical approximations made</li> <li>computational efficiency of the method</li> <li>transferability</li> <li>usability</li> </ul> </li> </ul> <h2 id="mm-based-simulations">MM-based simulations</h2> <ul> <li>often derived from experimental input</li> <li>computationally efficient 👍</li> <li>can handle larger systems when combined with coarse-grain approaches 👍</li> <li>the results are not generalizable 😥</li> <li>cannot describe the breaking and forming of chemical bonds 😥</li> </ul> <h2 id="qm-based-simulations">QM-based simulations</h2> <ul> <li>Solves the Schrodinger equation using DFT</li> <li>can be applied to all chemical species 👍</li> <li>can obtain a large set of material properties 👍</li> <li>high computational cost 😥</li> <li>cannot be used for larger systems 😥</li> </ul> <h2 id="overcoming-the-limitations-of-qm-based-simulations-using-machine-learning">Overcoming the limitations of QM-based simulations using machine learning</h2> <ul> <li>Takes the form of one of 4 methods: <ol> <li>Extension of the applicability range to QM simulations to larger length and time scales</li> <li>Prediction of properties calculated from QM methods</li> <li>Automated analysis of simulation data</li> <li>Inversion of atomistic calculations to generate atomic structures for a given set of properties</li> </ol> </li> </ul> <p>Strategy 1</p> <ul> <li>construct MLP(machine-learning potentials)</li> <li>computational cost goes from $O(n^3)$ to $O(n)$</li> </ul> <p>Strategy 2</p> <ul> <li>Trained to yield the outcome of QM-based calculations from atomistic simulations results</li> <li>less general than MLPs by construction</li> </ul> <p>Strategy 3</p> <ul> <li>used for automatic ID of crystal structures etc.</li> </ul> <p>Strategy 4</p> <ul> <li>currently in its infancy</li> <li>are not yet standardized and usability is therefore generally not yet given</li> </ul>]]></content><author><name></name></author><category term="research"/><category term="machine-learning"/><category term="quantum-mechanics"/><category term="simulation"/><category term="korean"/><summary type="html"><![CDATA[ML-accelerated QM simulations for industrial applications 논문 요약]]></summary></entry><entry><title type="html">현의 진동과 에너지</title><link href="https://codingjang.github.io/blog/2024/string-vibration-energy/" rel="alternate" type="text/html" title="현의 진동과 에너지"/><published>2024-03-15T10:00:00+00:00</published><updated>2024-03-15T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/string-vibration-energy</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/string-vibration-energy/"><![CDATA[<ul> <li>이 글은 영국 옥스퍼드 대학교 물리학과 강사 Dr. Christopher W. P. Palmer의 부교재 “Energy in Waves on Strings”를 번역한 것임을 알립니다. <a href="https://users.physics.ox.ac.uk/~palmerc/Wavesfiles/Energy_Handout.pdf">https://users.physics.ox.ac.uk/~palmerc/Wavesfiles/Energy_Handout.pdf</a> 오역을 발견할 시에는 jangyejun@snu.ac.kr로 연락 부탁드립니다.</li> </ul> <p>파동의 가장 특징적인 성질 중 하나는 에너지를 전달할 수 있다는 것이다. 이 부교재에서는 팽팽한 현에서의 에너지 전달 및 저장에 대해 분석할 것이다. 우선 현의 질량 선밀도가 $\rho$, 장력이 $T$일 것을 가정하면 파동의 속력은 $c=\sqrt{\frac{T}{\rho}}$로 구할 수 있다. 또한 현 위에서의 위치를 $x$좌표로 나타내고, 횡 방향으로의 변위를 $y$로 나타내면 아래의 파동 방정식을 만족한다.</p> \[\frac{\partial^2 y}{\partial x^2} = \frac{1}{c^2}\frac{\partial^2 y}{\partial t^2} \tag{1}\] <h2 id="1-운동-에너지-밀도">1. 운동 에너지 밀도</h2> <p>파동의 총 에너지는 현 위의 각각의 점에 대해 대응하는 에너지 선밀도 $u$로 분포되어 있다. 따라서 총 에너지 $E$는</p> \[E= \int{u(x,t)}\textrm{d}x \tag{2}\] <p>와 같이 나타낼 수 있다. 위 식에서의 적분은 현 전체에 대한 적분을 의미한다.</p> <p>에너지 밀도는 두 가지의 요소 - 운동 에너지 밀도와 퍼텐셜 에너지 밀도로 구성된다. 운동 에너지 밀도는 단위 길이 당 운동 에너지이며, 횡 방향 운동만을 고려하여 구할 수 있다. 길이가 $\delta x$인 현의 한 조각은 질량 $\rho\delta x$와 횡 방향 속도 $\partial y / \partial t$를 가지고, 따라서 운동 에너지</p> \[\frac{1}{2}\, \rho \delta x\!\left(\frac{\partial y}{\partial t}\right)^2\] <p>를 가진다. 따라서 운동 에너지 밀도 $u_K$는 아래와 같다.</p> \[\frac{1}{2} \,\rho\!\left(\frac{\partial y}{\partial t}\right)^2 \tag{3}\] <h2 id="2-퍼텐셜-에너지-밀도">2. 퍼텐셜 에너지 밀도</h2> <h3 id="간단한-접근">간단한 접근.</h3> <p>현의 횡 방향 운동이 장력에 주는 영향을 무시할 수 있다고 가정하면 현의 변형에는 일정한 장력에 대한 일이 필요할 것이다. 그렇다면 길이가 $\delta x$인 현의 한 조각에 저장된 에너지는 이 일정한 장력에 조각이 늘어난 길이를 곱한 것이다. 따라서 파동이 지나갈 때 이 조각의 길이가 $\delta l$이 된다고 하면 저장된 에너지는</p> \[u_P \delta x = T(\delta l - \delta x) \tag{4}\] <p>이다. 파동이 없을 때는 조각의 양 끝이 $(x, 0)$과 $(x+\delta x, 0)$에 놓여 있고, 파동에 의해 각각 $(x, y)$와 $(x+\delta x, y+\delta y)$로 옮겨진다고 하자. 그러면 조각의 변형된 길이는 아래와 같다.</p> \[\delta l = \sqrt{\delta x^2 + \delta y^2} = \delta x \sqrt{1+\left(\frac{\partial y}{\partial x}\right)^2}\] <p>파동을 다루는 전체 과정에서 기본이 되는 가정은 $\partial y / \partial x \ll 1$인 것이므로, 이항 전개에서 처음 두 항만을 남겨 위 식을</p> \[\delta l = \delta x \left(1+\frac{1}{2}\!\left(\frac{\partial y}{\partial x}\right)^2\right)\] <p>와 같이 근사할 수 있다. 이를 식 $(4)$에 대입하면 아래의 퍼텐셜 에너지 밀도를 얻는다.</p> \[u_P(x)=\frac{1}{2}T\left(\frac{\partial y}{\partial x}\right)^2 \tag{5}\] <p>앞서 언급한 두 요소를 조합하면 에너지 밀도는 아래와 같다:</p> \[u(x)=\frac{1}{2} \left(\rho \left(\frac{\partial y}{\partial t} \right)^2 + T \left(\frac{\partial y}{\partial x} \right)^2 \right) \tag{A}\;\;\;\;\textrm{(Energy\;Density)}\] <h3 id="더-자세한-접근"><strong>더 자세한 접근.</strong></h3> <p>$u_P$를 계산하기 위해 앞서 제시한 간단한 접근은 $T$가 상수라는 가정이 어떤 조건에서 타당한지 모르고, 특히 어떤 길이의 변화도 $T$에 조금은 영향을 주기 때문에 다소 부적절한 접근이라고 할 수 있다. 이 상황을 더 자세하게 설명하려면 한 발짝 뒤로 물러서서 늘어난 현의 탄성을 더 일반적으로 고려해야 한다.</p> <p>앞서 언급한 현의 한 조각의 길이가 $\delta x_0$라고 하자. 만약 이 조각이 임의의 길이 $\delta l$로 늘어난다고 하면, 장력 $T(\delta l)$과 저장된 에너지 $\delta U_P(\delta l)$는 아래의 당연한 공식으로 주어진다.</p> <p><em>(번역 주: 아래 공식이 당연한지는 독자가 알아서 판단하기를 바람)</em></p> \[T(\delta l) = \lambda \frac{\delta l - \delta x_0}{\delta x_0} \;\;\;\textrm{and} \;\;\;\delta U_P(\delta l)= \frac{\lambda}{2\delta x_0}(\delta l - \delta x_0)^2\] <p>여기서 $\lambda$는 현의 영률(원문: elastic constant)이다. 서로 다른 늘어난 상태 사이의 $\delta U_P$의 변화를 합차 공식을 이용하여 구하면</p> \[\delta U_P(\delta l_2)-\delta U_P(\delta l_1) = \frac{\lambda}{2\delta x_0} (\delta l_2 + \delta l_1 - 2 \delta x_0)(\delta l_2 - \delta l_1)\] <p>이고, 장력에 대한 공식을 위 식에 대입하면, 다시 아래와 같이 나타낼 수 있다.</p> \[\delta U_P(\delta l_2)-\delta U_P(\delta l_1) = \frac{T(\delta l_2 ) + T(\delta l_1)}{2}(\delta l_2 - \delta l_1)\] <p>구체적으로, 파동에 의해 추가적으로 생긴 탄성 퍼텐셜 에너지는 $\delta x$에서 $\delta l$로 늘리면서 생긴 차이이다.</p> \[u_P \delta x = \frac{T(\delta l)+T(\delta x)}{2} (\delta l - \delta x)\] <p>한편 간단한 접근에서 얻은 결과는 식 (4)로, 이 표기로는 아래와 같다.</p> \[u_P \delta x = T(\delta x) (\delta l - \delta x)\] <p>따라서 새로운 결과를 다시 써서 간단한 접근을 수정하면</p> \[\begin{align} u_P \delta x &amp;= T(\delta x)(\delta l - \delta x)+\frac{T(\delta l) - T(\delta x)}{2}(\delta l - \delta x) \newline &amp;= T(\delta x)\! \left((\delta l - \delta x)+\frac{1}{2}\frac{(\delta l - \delta x)^2}{(\delta x - \delta x_0)} \right) \end{align} \tag{6}\] <p>간단한 접근이 위 식에서 더 개선되었으나, 추가적인 길이 변화에 의한 효과는 기존 식보다 차수가 더 높고, 따라서 차수가 가장 낮은 항만 고려하면 식 (5)에서 구한 결과도 충분히 타당하다는 것을 확인할 수 있다.</p> <h3 id="단방향-파동에-대한-단순화"><strong>단방향 파동에 대한 단순화.</strong></h3> <p>파동 $y(x, t)$가 단순히 앞으로 진행하는 파동 $y=f(x-ct)$으로 구성되는 영역에서는 중요한 단순화가 일어난다. 이 영역에서의 파동은 아래와 같은 단방향 파동 방정식을 만족하게 된다.</p> \[\frac{\partial y}{\partial x} = - \frac{1}{c} \frac{\partial y}{\partial t}\] <p>이 경우에</p> \[u_P = \frac{1}{2}T\left(\frac{\partial y}{\partial x}\right)^2 = \frac{1}{2}T\left(-\frac{1}{c}\frac{\partial y}{\partial t}\right)^2 = \frac{1}{2} \frac{T}{c^2} \left(\frac{\partial y}{\partial t} \right)^2 = u_K \tag{7}\] <p>임을 보일 수 있다. 따라서 앞으로 진행하는 파동의 에너지 밀도는 $u_P$와 $u_K$에 동등하게 분할되며, 총 에너지는 아래 식으로 구할 수 있다.</p> \[u^+ = T\left(\frac{\partial y}{\partial x}\right)^2 = \rho\left(\frac{\partial y}{\partial t}\right)^2\] <p>비슷한 방식으로 $y=f(x+ct)$를 만족하는 영역에서는 뒤로 진행하는 파동의 방정식을 만족하며,</p> \[\frac{\partial y}{\partial x} = + \frac{1}{c} \frac{\partial y}{\partial t}\] <p>다시 한 번 $u_P = u_K$라는 결과와 총 에너지 밀도의 식을 얻는다.</p> \[u^- = T\left(\frac{\partial y}{\partial x}\right)^2 = \rho\left(\frac{\partial y}{\partial t}\right)^2\] <p>위에서 살펴 본 두 경우에 대해서 에너지 밀도에 대한 공식이 $u^+=u^-$로 같게 나왔지만, 이것은 앞으로 진행하는 파동과 뒤로 진행하는 파동이 동시에 존재할 경우에는 <em>사실이 아니다.</em></p> <h2 id="3-에너지-선속">3. 에너지 선속</h2> <p>처음에 언급했다시피, 파동의 특징적인 성질 중 하나는 에너지를 전달할 수 있는 것이다. 이것은 에너지 밀도의 존재만으로는 설명할 수 없으며, 에너지의 전달률인 선속 $\mathcal{F}$를 구해야 설명할 수 있다.</p> <p>여러 파동이 현을 따라 진행하고 있다고 하자. 목표는 점 $x$에서 $x$가 증가하는 방향으로 전달되는 에너지의 전달률인 $\mathcal{F}(x)$를 계산하는 것이다. 즉, 시간 당 $\mathcal{F}$만큼 음의 방향으로 놓인 현(음의 반현)은 에너지를 잃고, 양의 방향으로 놓인 현(양의 반현)은 에너지를 얻고 있는 것이다. 이것이 일어나는 이유는 음의 반현이 양의 반현에 $\mathcal{F}$의 일률로 일을 하기 때문이다. 이 점에서 현이 평형 상태의 방향을 기준으로 각도 $\theta$를 이루고 있다면, 현의 기울기는 $\tan \theta = \partial y /\partial x$이다. 따라서 음의 반현이 양의 반현에 작용하는 힘은</p> \[\textbf{F}= \begin{pmatrix} -T\cos \theta \\ -T \sin \theta \end{pmatrix}\] <p>이고, 일률은 힘과 힘의 작용점이 움직이는 속도</p> \[\textbf{v} = \begin{pmatrix} 0 \\ \frac{\partial y}{\partial t} \end{pmatrix}\] <p>의 내적이므로, $\mathcal{F}= \textbf{F}\cdot\textbf{v} = -T\sin\theta \frac{\partial y}{\partial t}$이다. 작은 각에 대한 근사 $\sin\theta \approx \tan\theta$, $\cos \theta \approx 1$을 사용하면 드디어 에너지 선속을 구하는 공식을 얻는다.</p> \[\mathcal{F}=-T\frac{\partial y}{\partial x}\frac{\partial y}{\partial t}\;\;\;\;\textrm{(Energy\;Flux\;Definition)} \tag{B}\] <p>에너지 밀도 $u$의 경우에서와 같이 앞으로 진행하거나 뒤로 진행하는 파동만이 존재할 때 위의 식을 단순화할 수 있다. 앞으로 진행하는 파동만이 존재한다면 다음의 단방향 파동 방정식이 성립한다.</p> \[\frac{\partial y}{\partial t} = -c \frac{\partial y}{\partial x}\] <p>이를 $\mathcal{F}$의 식에 대입하면</p> \[\mathcal{F}^+ = cT\left(\frac{\partial y}{\partial x}\right)^2 = cu^+\] <p>를 얻는다. 비슷하게, 뒤로 진행하는 파동만이 존재할 경우</p> \[\frac{\partial y}{\partial t}=c\frac{\partial y}{\partial x}\] <p>를 대입할 수 있고, 곧</p> \[\mathcal{F}^- = -cT\left(\frac{\partial y}{\partial x}\right)^2 = -cu^-\] <p>를 얻는다. 따라서 파동이 한 방향으로 진행하고 있는 영역이라면, 앞으로 진행하는 경우의 에너지 선속은 에너지 밀도 곱하기 파동의 진행 속력 $+c$이고, 반대로 진행하는 경우에는 $-c$를 곱해서 얻을 수 있다. 이 결과는 에너지가 위치에 의해 결정되는 특정 에너지 밀도 $u^{\pm}$의 형태로 파형 안에 잠재되어 있고 이것이 파동의 진행 속력으로 전달됨을 암시한다. 이것에 대한 추가적인 개념은 마지막 절에서 학습할 것이다.</p> <h2 id="4-에너지의-보존">4. 에너지의 보존</h2> <p>범우주적 에너지 보존의 관점에서 바라보면, 그리고 지금껏 세운 수식으로부터 파동 에너지가 현을 떠나 주변 환경으로 이동할 방법이 전혀 없다는 것을 고려한다면 전체 파동 에너지 $E$가 보존되기를 기대할 수 있다. 사실은 그것보다 더 많은 것을 기대한다. 앞서 살펴보았듯이 $E$는 실을 따라 분포해 있는 각각의 에너지 요소 $u$가 기여한 결과이고, 우리는 이미 위치에 따른 에너지 전달률의 표현식을 알고 있다. 따라서 우리는 에너지가 <em>국소적으로 보존(원문: locally conserved)</em>될 것임을 기대할 수 있다. 이것은 조각의 양 끝에서의 0이 아닌 에너지 선속에 의해, 임의의 현 조각이 지닌 에너지가 <strong>오직</strong> 인접한 현 조각으로 흐르면서 변화함을 의미한다. 이 가정은 u와 $\mathcal{F}$를 연결짓는 미분방정식을 유도할 수 있게 도와준다.</p> <p>x=a에서 x=b까지 늘어져 있는 현의 한 조각을 생각하자. 이 조각이 지니고 있는 에너지는</p> \[E_{a \rarr b} = \int_{a}^{b}{u(x, t)}\textrm{d}x\] <p>이다. 앞서 가정한 국소 에너지 보존은 위에서 구한 에너지가 오직 한 쪽 끝에서 들어오는 에너지 선속 $\mathcal{F}(a)$와 다른 쪽 끝에서 나가는 에너지 선속 $\mathcal{F}(b)$에 의해서만 변한다는 것을 의미한다.</p> \[\frac{\textrm{d}E_{a \rarr b}}{\textrm{d}t} = \mathcal{F}(a)-\mathcal{F}(b) \tag{8}\] <p>이것은 현의 어느 조각에 대해서도 성립하나, 두 개의 짧은 조각으로부터 하나의 긴 조각을 구성하는 경우를 생각해볼 수도 있다. 이 경우 각각의 짧은 조각에 대한 두 등식을 더하면 경계에서의 효과는 서로 상쇄되고 두 에너지의 합의 시간에 대한 변화율은 여전히 긴 조각의 양 끝에서의 에너지 선속 차이로 결정된다는 것을 확인할 수 있다. 따라서 중심이 $x$에 위치한 길이가 $\delta x$인, $x-\delta x/2$와 $x+\delta x/2$ 사이에 위치한 매우 짧은 조각을 생각할 수 있다. 충분히 짧은 조각이라면 이 구간 안에서 $u$의 변화는 무시할 수 있고, 따라서 조각이 지닌 에너지는 그저 $u \delta x$이다. 수식 $(8)$을 적용하면 다음을 얻는다.</p> \[\frac{\partial u}{\partial t}\delta x = \mathcal{F}(x-\delta x/2)-\mathcal{F}(x+\delta x/2) \tag{9}\] <p>테일러 전개에서 일차 항까지 취하면</p> \[\mathcal{F}(x+\delta x/2) = \mathcal{F}(x)+(\delta x/2) \frac{\partial \mathcal{F}}{\partial x}\] <p>이고, $\mathcal{F}(x-\delta x/2)$에 대해서도 비슷하게 구할 수 있다. 수식 (9)에 이를 대입하면 아래를 얻는다.</p> \[\frac{\partial u}{\partial t}=-\frac{\partial \mathcal{F}}{\partial x}\;\;\;\;\textrm{(Equation of Continuity)} \tag{C}\] <p>이것은 연속 방정식으로 널리 알려진, 방정식의 한 특수한 경우이지만, 지금은 이 이름이 실제 내용과 그다지 관련이 없는 쓸모없는 이름으로 보일 것이다. 이 방정식은 무엇인가의 국소 보존을 의미하는데, 이 경우에는 그 ‘무엇인가’가 에너지이다.</p>]]></content><author><name></name></author><category term="physics"/><category term="physics"/><category term="waves"/><category term="energy"/><category term="translation"/><category term="korean"/><summary type="html"><![CDATA[옥스퍼드 대학교 물리학 강의 자료 번역 - 파동의 에너지 전달과 저장]]></summary></entry><entry><title type="html">강화학습의 기초</title><link href="https://codingjang.github.io/blog/2024/reinforcement-learning-basics/" rel="alternate" type="text/html" title="강화학습의 기초"/><published>2024-02-01T10:00:00+00:00</published><updated>2024-02-01T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/reinforcement-learning-basics</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/reinforcement-learning-basics/"><![CDATA[<h2 id="deepmind-x-ucl-deep-rl-series">DeepMind X UCL Deep RL Series</h2> <table> <tbody> <tr> <td>[DeepMind X UCL</td> <td>4. Theoretical Fundamentals of Dynamic Programming](/assets/blog/reinforcement-learning-basics/DeepMind%20X%20UCL%204%20Theoretical%20Fundamentals%20of%20Dynam%207478da99308c409ebe009bf4f8209c13.md)</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>[DeepMind X UCL</td> <td>5. Model-free Prediction](/assets/blog/reinforcement-learning-basics/DeepMind%20X%20UCL%205%20Model-free%20Prediction%2094516d82c58143ba9ac04f5d72e72fcc.md)</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>[DeepMind X UCL</td> <td>6. Model-free Control](/assets/blog/reinforcement-learning-basics/DeepMind%20X%20UCL%206%20Model-free%20Control%20c55a856c97414c309f4e93dff6774282.md)</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>[DeepMind X UCL</td> <td>7. Function Approximation](/assets/blog/reinforcement-learning-basics/DeepMind%20X%20UCL%207%20Function%20Approximation%2086cf033e13e0489a902a735dba94a33c.md)</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>[DeepMind X UCL</td> <td>8. Planning and Models](/assets/blog/reinforcement-learning-basics/DeepMind%20X%20UCL%208%20Planning%20and%20Models%201d1f0f24f9318074b36ff0f1e69098a4.md)</td> </tr> </tbody> </table> <h2 id="etc">etc.</h2> <p><a href="/assets/blog/reinforcement-learning-basics/Introduction%20to%20PettingZoo%201f62ce393bc3449abd16466e77e86746.md">Introduction to PettingZoo</a></p>]]></content><author><name></name></author><category term="education"/><category term="reinforcement-learning"/><category term="deep-learning"/><category term="DeepMind"/><category term="tutorial"/><category term="korean"/><summary type="html"><![CDATA[DeepMind X UCL 강화학습 시리즈 노트 및 PettingZoo 소개]]></summary></entry><entry><title type="html">딥러닝의 기초</title><link href="https://codingjang.github.io/blog/2024/deep-learning-basics/" rel="alternate" type="text/html" title="딥러닝의 기초"/><published>2024-01-01T10:00:00+00:00</published><updated>2024-01-01T10:00:00+00:00</updated><id>https://codingjang.github.io/blog/2024/deep-learning-basics</id><content type="html" xml:base="https://codingjang.github.io/blog/2024/deep-learning-basics/"><![CDATA[<h3 id="english-version">English Version</h3> <p><a href="/assets/blog/deep-learning-basics/Introduction%20to%20Deep%20Learning%20957e35bcecd448278c201480cee70fab.md">Introduction to Deep Learning</a></p> <h2 id="part-i-선형대수학">Part I. 선형대수학</h2> <p><a href="/assets/blog/deep-learning-basics/Day%201%20%EB%B2%A1%ED%84%B0%EB%9E%80%20161f0f24f93180618f5bc0ebddbbd748.md">Day 1: 벡터란?</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%202%20%EA%B0%81%EC%A2%85%20%EA%B8%B0%EC%B4%88%20%EA%B0%9C%EB%85%90%EB%93%A4%20161f0f24f931803b9026f3a6ee6ca1ff.md">Day 2: 각종 기초 개념들</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%203%20%ED%96%89%EB%A0%AC%EA%B3%B1%EC%9D%98%20%EC%9D%98%EB%AF%B8%20161f0f24f9318086b671db43175227a7.md">Day 3: 행렬곱의 의미</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%204%20%EB%8B%A4%EC%B0%A8%EC%9B%90%20%EC%84%A0%ED%98%95%20%EB%B3%80%ED%99%98%20161f0f24f93180598c58f7c7de148e6d.md">Day 4: 다차원 선형 변환</a></p> <h2 id="part-ii-코딩">Part II. 코딩</h2> <p><a href="/assets/blog/deep-learning-basics/Day%205%20%EA%B8%B0%EC%B4%88%20%ED%8C%8C%EC%9D%B4%EC%8D%AC%20%EC%97%B0%EC%8A%B5%20I%20161f0f24f931805986e1c52f76535885.md">Day 5: 기초 파이썬 연습 I</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%206%20%EA%B8%B0%EC%B4%88%20%ED%8C%8C%EC%9D%B4%EC%8D%AC%20%EC%97%B0%EC%8A%B5%20II%20161f0f24f931807386a5ea9fed707d58.md">Day 6: 기초 파이썬 연습 II</a></p> <h2 id="part-iii-최적화의-기초">Part III. 최적화의 기초</h2> <p><a href="/assets/blog/deep-learning-basics/Day%207%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EB%AC%B8%EC%A0%9C%EC%9D%98%20%EC%A0%95%EC%9D%98%20161f0f24f931807abb21e857e37aa7de.md">Day 7: 최적화 문제의 정의</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%208%20%EC%9D%BC%EB%B3%80%EC%88%98%20%EC%B5%9C%EC%A0%81%ED%99%94%20%EB%AC%B8%EC%A0%9C%EC%99%80%20%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%20161f0f24f9318032a355d6c77b9562e5.md">Day 8: 일변수 최적화 문제와 경사하강법</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%209%20%EA%B5%AD%EC%86%8C%20%EC%B5%9C%EC%86%8C%EC%A0%90%EA%B3%BC%20%EC%A0%84%EC%97%AD%20%EC%B5%9C%EC%86%8C%EC%A0%90%20161f0f24f931802a97f3e1b6d06aea24.md">Day 9: 국소 최소점과 전역 최소점</a></p> <h2 id="part-iv-인공신경망-이론">Part IV. 인공신경망 이론</h2> <p><a href="/assets/blog/deep-learning-basics/Day%2010%20%EB%8B%A4%EB%B3%80%EC%88%98%EB%A1%9C%EC%9D%98%20%ED%99%95%EC%9E%A5%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%20161f0f24f9318072b790d52afd98d2a8.md">Day 10: 다변수로의 확장과 인공신경망</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%2011%20%EB%8B%A4%EB%B3%80%EC%88%98%20%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%20161f0f24f93180359866c07a7ae951c1.md">Day 11: 다변수 경사하강법</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%2012%20%EC%97%AD%EC%A0%84%ED%8C%8C%EC%9D%98%20%EA%B0%9C%EB%85%90%20161f0f24f93180ab8f5ced5a5be27c0b.md">Day 12: 역전파의 개념</a></p> <p><a href="/assets/blog/deep-learning-basics/Day%2013%20%EC%97%AD%EC%A0%84%ED%8C%8C%EC%9D%98%20%EA%B3%84%EC%82%B0%20161f0f24f93180a4a025d44fc5edf9a8.md">Day 13: 역전파의 계산</a></p> <h2 id="part-v-인공신경망-설계-및-실습">Part V. 인공신경망 설계 및 실습</h2> <p><a href="/assets/blog/deep-learning-basics/Day%2014%20%ED%81%B4%EB%9E%98%EC%8A%A4%20%EC%A0%95%EC%9D%98%20%EB%B0%8F%20%ED%8C%A8%ED%82%A4%EC%A7%80%20%EC%82%AC%EC%9A%A9%EB%B2%95%20161f0f24f93180418cf1c21b92e48445.md">Day 14: 클래스 정의 및 패키지 사용법</a></p>]]></content><author><name></name></author><category term="education"/><category term="deep-learning"/><category term="neural-networks"/><category term="machine-learning"/><category term="tutorial"/><category term="korean"/><summary type="html"><![CDATA[딥러닝을 처음 배우는 사람을 위한 종합 가이드 - 선형대수부터 역전파까지]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://codingjang.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://codingjang.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://codingjang.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>